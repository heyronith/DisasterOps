{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Step 1: PDF Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install pdfplumber"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raw directory: /Users/ronny/Desktop/ML projects/DisasterOps/Data/raw\n",
            "Processed directory: /Users/ronny/Desktop/ML projects/DisasterOps/Data/processed\n",
            "Metadata directory: /Users/ronny/Desktop/ML projects/DisasterOps/Data/metadata\n"
          ]
        }
      ],
      "source": [
        "import pdfplumber\n",
        "import json\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional\n",
        "from collections import defaultdict\n",
        "\n",
        "BASE_DIR = Path.cwd()\n",
        "RAW_DIR = BASE_DIR / \"Data\" / \"raw\"\n",
        "PROCESSED_DIR = BASE_DIR / \"Data\" / \"processed\"\n",
        "METADATA_DIR = BASE_DIR / \"Data\" / \"metadata\"\n",
        "\n",
        "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
        "METADATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Raw directory: {RAW_DIR}\")\n",
        "print(f\"Processed directory: {PROCESSED_DIR}\")\n",
        "print(f\"Metadata directory: {METADATA_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_citation_prefix(pdf_path: Path) -> str:\n",
        "    \"\"\"Generate citation prefix from file path\"\"\"\n",
        "    category = pdf_path.parent.name\n",
        "    filename = pdf_path.stem.lower()\n",
        "    \n",
        "    if category == \"ics_forms\":\n",
        "        # Extract form number (e.g., \"ics_form_201\" -> \"fema_ics201\")\n",
        "        match = re.search(r'ics_form_(\\d+)', filename)\n",
        "        if match:\n",
        "            return f\"fema_ics{match.group(1)}\"\n",
        "        elif \"booklet\" in filename:\n",
        "            return \"fema_ics_forms_booklet\"\n",
        "        elif \"reference\" in filename:\n",
        "            return \"fema_ics_reference\"\n",
        "    elif category == \"cert\":\n",
        "        return \"cert_basic_training_manual\"\n",
        "    elif category == \"ready_gov\":\n",
        "        # Map filenames to citation prefixes\n",
        "        if \"are_you_ready\" in filename:\n",
        "            return \"ready_gov_are_you_ready\"\n",
        "        elif \"caregivers\" in filename:\n",
        "            return \"ready_gov_caregivers\"\n",
        "        elif \"earthquake\" in filename:\n",
        "            return \"ready_gov_earthquake\"\n",
        "        elif \"flood\" in filename:\n",
        "            return \"ready_gov_flood\"\n",
        "        elif \"hurricane\" in filename:\n",
        "            return \"ready_gov_hurricane\"\n",
        "        elif \"tornado\" in filename:\n",
        "            return \"ready_gov_tornado\"\n",
        "        elif \"wildfire\" in filename:\n",
        "            return \"ready_gov_wildfire\"\n",
        "    elif category == \"fema_other\":\n",
        "        if \"nims_doctrine\" in filename:\n",
        "            return \"fema_nims_doctrine\"\n",
        "        elif \"nrf\" in filename:\n",
        "            return \"fema_nrf\"\n",
        "        elif \"nims\" in filename:\n",
        "            return f\"fema_nims_{filename.replace('nims-', '').replace('_', '_')}\"\n",
        "    \n",
        "    return f\"{category}_{filename}\"\n",
        "\n",
        "def extract_sections(text: str) -> List[Dict]:\n",
        "    \"\"\"Extract sections based on headers and track page numbers\"\"\"\n",
        "    lines = text.split('\\n')\n",
        "    sections = []\n",
        "    current_section = {\"title\": \"Introduction\", \"content\": [], \"page\": 1}\n",
        "    current_page = 1\n",
        "    \n",
        "    # Patterns for section headers\n",
        "    header_patterns = [\n",
        "        r'^(Chapter\\s+\\d+|Section\\s+\\d+|Unit\\s+\\d+)',\n",
        "        r'^[A-Z][A-Z\\s]{10,}$',  # ALL CAPS lines\n",
        "        r'^\\d+\\.\\s+[A-Z]',  # Numbered sections\n",
        "        r'^[IVX]+\\.\\s+[A-Z]',  # Roman numerals\n",
        "    ]\n",
        "    \n",
        "    for line in lines:\n",
        "        # Extract page number from markers\n",
        "        page_match = re.search(r'--- Page (\\d+) ---', line)\n",
        "        if page_match:\n",
        "            current_page = int(page_match.group(1))\n",
        "            continue\n",
        "        \n",
        "        line_stripped = line.strip()\n",
        "        if not line_stripped:\n",
        "            if current_section[\"content\"]:\n",
        "                current_section[\"content\"].append(\"\")\n",
        "            continue\n",
        "        \n",
        "        # Check if line looks like a header\n",
        "        is_header = any(re.match(pattern, line_stripped) for pattern in header_patterns)\n",
        "        \n",
        "        if is_header and len(line_stripped) < 100 and len(current_section[\"content\"]) > 5:\n",
        "            # Save current section\n",
        "            sections.append({\n",
        "                \"title\": current_section[\"title\"],\n",
        "                \"content\": \"\\n\".join(current_section[\"content\"]).strip(),\n",
        "                \"page\": current_section[\"page\"]\n",
        "            })\n",
        "            # Start new section\n",
        "            current_section = {\"title\": line_stripped, \"content\": [], \"page\": current_page}\n",
        "        else:\n",
        "            current_section[\"content\"].append(line_stripped)\n",
        "            if current_section[\"page\"] == 1 and current_page > 1:\n",
        "                current_section[\"page\"] = current_page\n",
        "    \n",
        "    # Add final section\n",
        "    if current_section[\"content\"]:\n",
        "        sections.append({\n",
        "            \"title\": current_section[\"title\"],\n",
        "            \"content\": \"\\n\".join(current_section[\"content\"]).strip(),\n",
        "            \"page\": current_section[\"page\"]\n",
        "        })\n",
        "    \n",
        "    return sections if sections else [{\"title\": \"Full Document\", \"content\": text, \"page\": 1}]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_pdf(pdf_path: Path) -> Dict:\n",
        "    \"\"\"Process a single PDF and extract structured content\"\"\"\n",
        "    citation_prefix = get_citation_prefix(pdf_path)\n",
        "    print(f\"Processing: {pdf_path.name} -> {citation_prefix}\")\n",
        "    \n",
        "    full_text = \"\"\n",
        "    pages_content = []\n",
        "    \n",
        "    try:\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            for page_num, page in enumerate(pdf.pages, 1):\n",
        "                text = page.extract_text() or \"\"\n",
        "                full_text += f\"\\n\\n--- Page {page_num} ---\\n\\n{text}\"\n",
        "                pages_content.append({\n",
        "                    \"page\": page_num,\n",
        "                    \"text\": text\n",
        "                })\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {pdf_path.name}: {e}\")\n",
        "        return None\n",
        "    \n",
        "    # Extract sections\n",
        "    sections = extract_sections(full_text)\n",
        "    \n",
        "    # Create citation mappings\n",
        "    citations = []\n",
        "    section_idx = 1\n",
        "    \n",
        "    for section in sections:\n",
        "        start_page = section.get(\"page\", 1)\n",
        "        citation_id = f\"{citation_prefix}_section{section_idx}_p{start_page}\"\n",
        "        citations.append({\n",
        "            \"citation_id\": citation_id,\n",
        "            \"section_title\": section[\"title\"],\n",
        "            \"section_number\": section_idx,\n",
        "            \"start_page\": start_page,\n",
        "            \"content_preview\": section[\"content\"][:200] + \"...\" if len(section[\"content\"]) > 200 else section[\"content\"]\n",
        "        })\n",
        "        section_idx += 1\n",
        "    \n",
        "    # Clean full_text (remove page markers for cleaner storage)\n",
        "    clean_text = re.sub(r'--- Page \\d+ ---\\n\\n', '\\n', full_text)\n",
        "    \n",
        "    return {\n",
        "        \"source_file\": str(pdf_path.relative_to(BASE_DIR)),\n",
        "        \"citation_prefix\": citation_prefix,\n",
        "        \"total_pages\": len(pages_content),\n",
        "        \"full_text\": clean_text,\n",
        "        \"pages\": pages_content,\n",
        "        \"sections\": sections,\n",
        "        \"citations\": citations,\n",
        "        \"metadata\": {\n",
        "            \"category\": pdf_path.parent.name,\n",
        "            \"filename\": pdf_path.name,\n",
        "            \"word_count\": len(clean_text.split()),\n",
        "            \"char_count\": len(clean_text)\n",
        "        }\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing: ics_form_209_incident_status_summary_v3.pdf -> fema_ics209\n",
            "Processing: ics_form_205a_communications_list_v3.pdf -> fema_ics205\n",
            "Processing: ics_form_211_incident_check-in_list_v3.1.pdf -> fema_ics211\n",
            "Processing: ics_form_207_incident_organization_chart_v3.pdf -> fema_ics207\n",
            "Processing: ics_form_213_general_message_v3.pdf -> fema_ics213\n",
            "Processing: ics_form_210_resource_status_change_v3.pdf -> fema_ics210\n",
            "Processing: ics_form_215_operational_planning_worksheet_v3.pdf -> fema_ics215\n",
            "Processing: ics_form_204_assignment_list_v3.1.pdf -> fema_ics204\n",
            "Processing: ics_form_201_incident_briefing_v3.pdf -> fema_ics201\n",
            "Processing: nims-guideline-resource-management-preparedness.pdf -> fema_nims_guideline-resource-management-preparedness\n",
            "Processing: nims_doctrine.pdf -> fema_nims_doctrine\n",
            "Processing: nims-incident-complexity-guide.pdf -> fema_nims_incident-complexity-guide\n",
            "Processing: nrf_finalapproved_2011028.pdf -> fema_nrf\n",
            "Processing: cert_basic_training_manual.pdf -> cert_basic_training_manual\n",
            "Processing: IS0100c_IG.pdf -> 04 IG_is0100c_ig\n",
            "Processing: IS0100c_POI.pdf -> 04 IG_is0100c_poi\n",
            "Processing: ICS_for_Schools.pdf -> 06 Handouts_ics_for_schools\n",
            "Processing: ICS_for_Utilities.pdf -> 06 Handouts_ics_for_utilities\n",
            "Processing: ICS_for_Public_Works.pdf -> 06 Handouts_ics_for_public_works\n",
            "Processing: IS0100c_SM.pdf -> 03 SM_is0100c_sm\n",
            "Processing: National_Incident_Management System_Third Edition_October_2017.pdf -> Handouts_national_incident_management system_third edition_october_2017\n",
            "Processing: POIIS0700b.pdf -> POI_poiis0700b\n",
            "Processing: IGIS0700b.pdf -> Instructor Guide_igis0700b\n",
            "Processing: SMIS0700b.pdf -> Student Manual_smis0700b\n",
            "Processing: IS0200c IG.pdf -> 04 IG_is0200c ig\n",
            "Processing: IS0200c_POI.pdf -> 04 IG_is0200c_poi\n",
            "Processing: Sample Typing (pdf).pdf -> 06 Handouts_sample typing (pdf)\n",
            "Processing: Activity 4.2 - ICS201 Student Version.pdf -> 06 Handouts_activity 4.2 - ics201 student version\n",
            "Processing: IS-200.c Sample Completed ICS Form 201.pdf -> 06 Handouts_is-200.c sample completed ics form 201\n",
            "Processing: IS0200c SM.pdf -> 03 SM_is0200c sm\n",
            "\n",
            "âœ… Processed: 49\n",
            "âŒ Failed: 0\n"
          ]
        }
      ],
      "source": [
        "# Process all PDFs\n",
        "all_pdfs = list(RAW_DIR.rglob(\"*.pdf\"))\n",
        "print(f\"Found {len(all_pdfs)} PDF files to process\\n\")\n",
        "\n",
        "processed_docs = []\n",
        "failed_docs = []\n",
        "\n",
        "for pdf_path in all_pdfs:\n",
        "    result = process_pdf(pdf_path)\n",
        "    if result:\n",
        "        processed_docs.append(result)\n",
        "        \n",
        "        # Save processed version\n",
        "        processed_file = PROCESSED_DIR / f\"{result['citation_prefix']}.json\"\n",
        "        with open(processed_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(result, f, indent=2, ensure_ascii=False)\n",
        "        \n",
        "        # Save metadata\n",
        "        metadata_file = METADATA_DIR / f\"{result['citation_prefix']}_metadata.json\"\n",
        "        metadata = {\n",
        "            \"citation_prefix\": result[\"citation_prefix\"],\n",
        "            \"source_file\": result[\"source_file\"],\n",
        "            \"total_pages\": result[\"total_pages\"],\n",
        "            \"num_sections\": len(result[\"sections\"]),\n",
        "            \"num_citations\": len(result[\"citations\"]),\n",
        "            \"metadata\": result[\"metadata\"]\n",
        "        }\n",
        "        with open(metadata_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
        "    else:\n",
        "        failed_docs.append(str(pdf_path))\n",
        "\n",
        "print(f\"\\nâœ… Processed: {len(processed_docs)}\")\n",
        "print(f\"âŒ Failed: {len(failed_docs)}\")\n",
        "if failed_docs:\n",
        "    print(\"\\nFailed files:\")\n",
        "    for f in failed_docs:\n",
        "        print(f\"  - {f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Citation index created with 754 entries\n",
            "ðŸ“ Saved to: /Users/ronny/Desktop/ML projects/DisasterOps/Data/metadata/citation_index.json\n",
            "\n",
            "ðŸ“Š Summary:\n",
            "  Documents: 49\n",
            "  Pages: 2714\n",
            "  Citations: 754\n",
            "  Words: 541,509\n",
            "  Categories: {'ready_gov': 7, 'ics_forms': 21, 'fema_other': 4, 'cert': 1, '04 IG': 4, '06 Handouts': 6, '03 SM': 2, 'Handouts': 1, 'POI': 1, 'Instructor Guide': 1, 'Student Manual': 1}\n"
          ]
        }
      ],
      "source": [
        "# Create master citation index\n",
        "citation_index = {}\n",
        "for doc in processed_docs:\n",
        "    for citation in doc[\"citations\"]:\n",
        "        citation_index[citation[\"citation_id\"]] = {\n",
        "            \"citation_prefix\": doc[\"citation_prefix\"],\n",
        "            \"source_file\": doc[\"source_file\"],\n",
        "            \"section_title\": citation[\"section_title\"],\n",
        "            \"section_number\": citation[\"section_number\"],\n",
        "            \"start_page\": citation[\"start_page\"],\n",
        "            \"content_preview\": citation[\"content_preview\"]\n",
        "        }\n",
        "\n",
        "# Save citation index\n",
        "index_file = METADATA_DIR / \"citation_index.json\"\n",
        "with open(index_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(citation_index, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"âœ… Citation index created with {len(citation_index)} entries\")\n",
        "print(f\"ðŸ“ Saved to: {index_file}\")\n",
        "\n",
        "# Create summary statistics\n",
        "summary = {\n",
        "    \"total_documents\": len(processed_docs),\n",
        "    \"total_pages\": sum(doc[\"total_pages\"] for doc in processed_docs),\n",
        "    \"total_citations\": len(citation_index),\n",
        "    \"categories\": defaultdict(int),\n",
        "    \"word_count\": sum(doc[\"metadata\"][\"word_count\"] for doc in processed_docs)\n",
        "}\n",
        "\n",
        "for doc in processed_docs:\n",
        "    summary[\"categories\"][doc[\"metadata\"][\"category\"]] += 1\n",
        "\n",
        "summary_file = METADATA_DIR / \"processing_summary.json\"\n",
        "with open(summary_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(summary, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"\\nðŸ“Š Summary:\")\n",
        "print(f\"  Documents: {summary['total_documents']}\")\n",
        "print(f\"  Pages: {summary['total_pages']}\")\n",
        "print(f\"  Citations: {summary['total_citations']}\")\n",
        "print(f\"  Words: {summary['word_count']:,}\")\n",
        "print(f\"  Categories: {dict(summary['categories'])}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RAG PIPELINE ( Chunking + embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Installing collected packages: rank-bm25, sentence-transformers\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2/2\u001b[0m [sentence-transformers]\n",
            "\u001b[1A\u001b[2KSuccessfully installed rank-bm25-0.2.2 sentence-transformers-5.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers rank-bm25 torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading embedding model...\n",
            "âœ“ Model loaded\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from rank_bm25 import BM25Okapi\n",
        "import pickle\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "# Paths\n",
        "CHUNKS_DIR = BASE_DIR / \"Data\" / \"chunks\"\n",
        "EMBEDDINGS_DIR = BASE_DIR / \"Data\" / \"embeddings\"\n",
        "CHUNKS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "EMBEDDINGS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Load embedding model (using sentence-transformers as default - no API key needed)\n",
        "print(\"Loading embedding model...\")\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')  # Fast, good quality, 384 dims\n",
        "print(\"âœ“ Model loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chunk_text(text: str, citation_id: str, chunk_size: int = 500, chunk_overlap: int = 50) -> List[Dict]:\n",
        "    \"\"\"Create chunks with overlap and preserve citation mapping\"\"\"\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    \n",
        "    for i in range(0, len(words), chunk_size - chunk_overlap):\n",
        "        chunk_words = words[i:i + chunk_size]\n",
        "        chunk_text = \" \".join(chunk_words)\n",
        "        \n",
        "        if chunk_text.strip():\n",
        "            chunks.append({\n",
        "                \"text\": chunk_text,\n",
        "                \"citation_id\": citation_id,\n",
        "                \"chunk_index\": len(chunks),\n",
        "                \"word_count\": len(chunk_words)\n",
        "            })\n",
        "    \n",
        "    return chunks\n",
        "\n",
        "def process_documents_for_rag(processed_docs: List[Dict]) -> Tuple[List[Dict], Dict]:\n",
        "    \"\"\"Create chunks from all processed documents\"\"\"\n",
        "    all_chunks = []\n",
        "    chunk_to_citation = {}\n",
        "    \n",
        "    for doc in processed_docs:\n",
        "        citation_prefix = doc[\"citation_prefix\"]\n",
        "        \n",
        "        # Chunk by sections (hierarchical approach)\n",
        "        for section in doc[\"sections\"]:\n",
        "            section_citation = f\"{citation_prefix}_section{section.get('section_number', 1)}_p{section.get('page', 1)}\"\n",
        "            section_text = section[\"content\"]\n",
        "            \n",
        "            # Further chunk if section is too long\n",
        "            if len(section_text.split()) > 500:\n",
        "                section_chunks = chunk_text(section_text, section_citation)\n",
        "                all_chunks.extend(section_chunks)\n",
        "            else:\n",
        "                all_chunks.append({\n",
        "                    \"text\": section_text,\n",
        "                    \"citation_id\": section_citation,\n",
        "                    \"chunk_index\": 0,\n",
        "                    \"word_count\": len(section_text.split())\n",
        "                })\n",
        "    \n",
        "    # Build citation mapping\n",
        "    for i, chunk in enumerate(all_chunks):\n",
        "        chunk_to_citation[i] = chunk[\"citation_id\"]\n",
        "    \n",
        "    return all_chunks, chunk_to_citation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading processed documents...\n",
            "Loaded 47 documents\n",
            "\n",
            "Creating chunks...\n",
            "âœ“ Created 1694 chunks\n",
            "âœ“ Average chunk size: 343 words\n",
            "âœ“ Saved chunks to /Users/ronny/Desktop/ML projects/DisasterOps/Data/chunks/all_chunks.json\n"
          ]
        }
      ],
      "source": [
        "# Load processed documents and create chunks\n",
        "print(\"Loading processed documents...\")\n",
        "processed_files = list(PROCESSED_DIR.glob(\"*.json\"))\n",
        "processed_docs = []\n",
        "for f in processed_files:\n",
        "    with open(f, 'r', encoding='utf-8') as file:\n",
        "        processed_docs.append(json.load(file))\n",
        "\n",
        "print(f\"Loaded {len(processed_docs)} documents\")\n",
        "\n",
        "print(\"\\nCreating chunks...\")\n",
        "all_chunks, chunk_to_citation = process_documents_for_rag(processed_docs)\n",
        "\n",
        "print(f\"âœ“ Created {len(all_chunks)} chunks\")\n",
        "print(f\"âœ“ Average chunk size: {np.mean([c['word_count'] for c in all_chunks]):.0f} words\")\n",
        "\n",
        "# Save chunks\n",
        "chunks_file = CHUNKS_DIR / \"all_chunks.json\"\n",
        "with open(chunks_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(all_chunks, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "citation_map_file = CHUNKS_DIR / \"chunk_to_citation.json\"\n",
        "with open(citation_map_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(chunk_to_citation, f, indent=2)\n",
        "\n",
        "print(f\"âœ“ Saved chunks to {chunks_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating embeddings...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b9eaffe4279d4ef79158373b6755d142",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/53 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Generated 1694 embeddings\n",
            "âœ“ Embedding dimension: 384\n",
            "âœ“ Saved embeddings to /Users/ronny/Desktop/ML projects/DisasterOps/Data/embeddings/embeddings.npy\n"
          ]
        }
      ],
      "source": [
        "# Generate embeddings\n",
        "print(\"Generating embeddings...\")\n",
        "chunk_texts = [chunk[\"text\"] for chunk in all_chunks]\n",
        "embeddings = embedding_model.encode(chunk_texts, show_progress_bar=True, batch_size=32)\n",
        "\n",
        "print(f\"âœ“ Generated {len(embeddings)} embeddings\")\n",
        "print(f\"âœ“ Embedding dimension: {embeddings.shape[1]}\")\n",
        "\n",
        "# Save embeddings\n",
        "embeddings_file = EMBEDDINGS_DIR / \"embeddings.npy\"\n",
        "np.save(embeddings_file, embeddings)\n",
        "print(f\"âœ“ Saved embeddings to {embeddings_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building BM25 index...\n",
            "âœ“ BM25 index built\n",
            "âœ“ Saved BM25 model to /Users/ronny/Desktop/ML projects/DisasterOps/Data/embeddings/bm25_model.pkl\n"
          ]
        }
      ],
      "source": [
        "# Build BM25 index (sparse retrieval)\n",
        "print(\"Building BM25 index...\")\n",
        "tokenized_chunks = [chunk[\"text\"].lower().split() for chunk in all_chunks]\n",
        "bm25 = BM25Okapi(tokenized_chunks)\n",
        "\n",
        "print(\"âœ“ BM25 index built\")\n",
        "\n",
        "# Save BM25 model\n",
        "bm25_file = EMBEDDINGS_DIR / \"bm25_model.pkl\"\n",
        "with open(bm25_file, 'wb') as f:\n",
        "    pickle.dump(bm25, f)\n",
        "print(f\"âœ“ Saved BM25 model to {bm25_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing search with query: 'How to establish triage area during emergency'\n",
            "\n",
            "Top 5 results:\n",
            "\n",
            "1. Score: 0.9291\n",
            "   Citation: fema_nims_doctrine_section1_p29\n",
            "   Text: incident management frequently necessitates ratios significantly different from this. The 1:5 ratio is a guideline, and incident personnel use their b...\n",
            "\n",
            "2. Score: 0.9291\n",
            "   Citation: Handouts_national_incident_management system_third edition_october_2017_section1_p29\n",
            "   Text: incident management frequently necessitates ratios significantly different from this. The 1:5 ratio is a guideline, and incident personnel use their b...\n",
            "\n",
            "3. Score: 0.9026\n",
            "   Citation: ready_gov_caregivers_section1_p17\n",
            "   Text: Sometimes the best way to stay safe in an emergency is to get inside and stay inside a building or vehicle until danger has passed. This is also known...\n",
            "\n",
            "4. Score: 0.8836\n",
            "   Citation: fema_ics_reference_section1_p13\n",
            "   Text: A temporary location managed by the Operations Section. A staging area can be any location in which personnel, supplies, and equipment await assignmen...\n",
            "\n",
            "5. Score: 0.8772\n",
            "   Citation: cert_basic_training_manual_section1_p234\n",
            "   Text: do so, take others with you. August 2019 Page 8-11 CERT Unit 8: Terrorism and CERT Participant Manual A bomb explosion can cause secondary explosions ...\n"
          ]
        }
      ],
      "source": [
        "def hybrid_search(query: str, embeddings: np.ndarray, bm25: BM25Okapi, chunk_texts: List[str], \n",
        "                  top_k: int = 10, dense_weight: float = 0.5) -> List[Tuple[int, float]]:\n",
        "    \"\"\"Hybrid search combining dense (embeddings) and sparse (BM25) retrieval\"\"\"\n",
        "    # Dense search\n",
        "    query_embedding = embedding_model.encode([query])[0]\n",
        "    dense_scores = np.dot(embeddings, query_embedding)\n",
        "    dense_scores = (dense_scores - dense_scores.min()) / (dense_scores.max() - dense_scores.min() + 1e-8)\n",
        "    \n",
        "    # Sparse search (BM25)\n",
        "    tokenized_query = query.lower().split()\n",
        "    sparse_scores = bm25.get_scores(tokenized_query)\n",
        "    sparse_scores = (sparse_scores - sparse_scores.min()) / (sparse_scores.max() - sparse_scores.min() + 1e-8)\n",
        "    \n",
        "    # Combine scores\n",
        "    hybrid_scores = dense_weight * dense_scores + (1 - dense_weight) * sparse_scores\n",
        "    \n",
        "    # Get top k\n",
        "    top_indices = np.argsort(hybrid_scores)[::-1][:top_k]\n",
        "    results = [(int(idx), float(hybrid_scores[idx])) for idx in top_indices]\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Test search\n",
        "test_query = \"How to establish triage area during emergency\"\n",
        "print(f\"Testing search with query: '{test_query}'\")\n",
        "results = hybrid_search(test_query, embeddings, bm25, chunk_texts, top_k=5)\n",
        "\n",
        "print(\"\\nTop 5 results:\")\n",
        "for i, (idx, score) in enumerate(results, 1):\n",
        "    chunk = all_chunks[idx]\n",
        "    print(f\"\\n{i}. Score: {score:.4f}\")\n",
        "    print(f\"   Citation: {chunk['citation_id']}\")\n",
        "    print(f\"   Text: {chunk['text'][:150]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ… RAG Pipeline Complete!\n",
            "ðŸ“Š Summary:\n",
            "   Chunks: 1694\n",
            "   Embeddings: (1694, 384)\n",
            "   BM25 index: Built\n",
            "   Metadata saved to: /Users/ronny/Desktop/ML projects/DisasterOps/Data/embeddings/rag_metadata.json\n"
          ]
        }
      ],
      "source": [
        "# Save RAG pipeline metadata\n",
        "rag_metadata = {\n",
        "    \"total_chunks\": len(all_chunks),\n",
        "    \"embedding_dim\": embeddings.shape[1],\n",
        "    \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
        "    \"chunk_size\": 500,\n",
        "    \"chunk_overlap\": 50,\n",
        "    \"files\": {\n",
        "        \"chunks\": str(chunks_file.relative_to(BASE_DIR)),\n",
        "        \"embeddings\": str(embeddings_file.relative_to(BASE_DIR)),\n",
        "        \"bm25\": str(bm25_file.relative_to(BASE_DIR)),\n",
        "        \"citation_map\": str(citation_map_file.relative_to(BASE_DIR))\n",
        "    }\n",
        "}\n",
        "\n",
        "rag_metadata_file = EMBEDDINGS_DIR / \"rag_metadata.json\"\n",
        "with open(rag_metadata_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(rag_metadata, f, indent=2)\n",
        "\n",
        "print(f\"\\nâœ… RAG Pipeline Complete!\")\n",
        "print(f\"ðŸ“Š Summary:\")\n",
        "print(f\"   Chunks: {rag_metadata['total_chunks']}\")\n",
        "print(f\"   Embeddings: {embeddings.shape}\")\n",
        "print(f\"   BM25 index: Built\")\n",
        "print(f\"   Metadata saved to: {rag_metadata_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (mlprojects)",
      "language": "python",
      "name": "mlprojects"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
