{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 1: PDF Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw directory: /Users/ronny/Desktop/ML projects/DisasterOps/Data/raw\n",
      "Processed directory: /Users/ronny/Desktop/ML projects/DisasterOps/Data/processed\n",
      "Metadata directory: /Users/ronny/Desktop/ML projects/DisasterOps/Data/metadata\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "from collections import defaultdict\n",
    "\n",
    "BASE_DIR = Path.cwd()\n",
    "RAW_DIR = BASE_DIR / \"Data\" / \"raw\"\n",
    "PROCESSED_DIR = BASE_DIR / \"Data\" / \"processed\"\n",
    "METADATA_DIR = BASE_DIR / \"Data\" / \"metadata\"\n",
    "\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "METADATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Raw directory: {RAW_DIR}\")\n",
    "print(f\"Processed directory: {PROCESSED_DIR}\")\n",
    "print(f\"Metadata directory: {METADATA_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_citation_prefix(pdf_path: Path) -> str:\n",
    "    \"\"\"Generate citation prefix from file path\"\"\"\n",
    "    category = pdf_path.parent.name\n",
    "    filename = pdf_path.stem.lower()\n",
    "    \n",
    "    if category == \"ics_forms\":\n",
    "        # Extract form number (e.g., \"ics_form_201\" -> \"fema_ics201\")\n",
    "        match = re.search(r'ics_form_(\\d+)', filename)\n",
    "        if match:\n",
    "            return f\"fema_ics{match.group(1)}\"\n",
    "        elif \"booklet\" in filename:\n",
    "            return \"fema_ics_forms_booklet\"\n",
    "        elif \"reference\" in filename:\n",
    "            return \"fema_ics_reference\"\n",
    "    elif category == \"cert\":\n",
    "        return \"cert_basic_training_manual\"\n",
    "    elif category == \"ready_gov\":\n",
    "        # Map filenames to citation prefixes\n",
    "        if \"are_you_ready\" in filename:\n",
    "            return \"ready_gov_are_you_ready\"\n",
    "        elif \"caregivers\" in filename:\n",
    "            return \"ready_gov_caregivers\"\n",
    "        elif \"earthquake\" in filename:\n",
    "            return \"ready_gov_earthquake\"\n",
    "        elif \"flood\" in filename:\n",
    "            return \"ready_gov_flood\"\n",
    "        elif \"hurricane\" in filename:\n",
    "            return \"ready_gov_hurricane\"\n",
    "        elif \"tornado\" in filename:\n",
    "            return \"ready_gov_tornado\"\n",
    "        elif \"wildfire\" in filename:\n",
    "            return \"ready_gov_wildfire\"\n",
    "    elif category == \"fema_other\":\n",
    "        if \"nims_doctrine\" in filename:\n",
    "            return \"fema_nims_doctrine\"\n",
    "        elif \"nrf\" in filename:\n",
    "            return \"fema_nrf\"\n",
    "        elif \"nims\" in filename:\n",
    "            return f\"fema_nims_{filename.replace('nims-', '').replace('_', '_')}\"\n",
    "    \n",
    "    return f\"{category}_{filename}\"\n",
    "\n",
    "def extract_sections(text: str) -> List[Dict]:\n",
    "    \"\"\"Extract sections based on headers and track page numbers\"\"\"\n",
    "    lines = text.split('\\n')\n",
    "    sections = []\n",
    "    current_section = {\"title\": \"Introduction\", \"content\": [], \"page\": 1}\n",
    "    current_page = 1\n",
    "    \n",
    "    # Patterns for section headers\n",
    "    header_patterns = [\n",
    "        r'^(Chapter\\s+\\d+|Section\\s+\\d+|Unit\\s+\\d+)',\n",
    "        r'^[A-Z][A-Z\\s]{10,}$',  # ALL CAPS lines\n",
    "        r'^\\d+\\.\\s+[A-Z]',  # Numbered sections\n",
    "        r'^[IVX]+\\.\\s+[A-Z]',  # Roman numerals\n",
    "    ]\n",
    "    \n",
    "    for line in lines:\n",
    "        # Extract page number from markers\n",
    "        page_match = re.search(r'--- Page (\\d+) ---', line)\n",
    "        if page_match:\n",
    "            current_page = int(page_match.group(1))\n",
    "            continue\n",
    "        \n",
    "        line_stripped = line.strip()\n",
    "        if not line_stripped:\n",
    "            if current_section[\"content\"]:\n",
    "                current_section[\"content\"].append(\"\")\n",
    "            continue\n",
    "        \n",
    "        # Check if line looks like a header\n",
    "        is_header = any(re.match(pattern, line_stripped) for pattern in header_patterns)\n",
    "        \n",
    "        if is_header and len(line_stripped) < 100 and len(current_section[\"content\"]) > 5:\n",
    "            # Save current section\n",
    "            sections.append({\n",
    "                \"title\": current_section[\"title\"],\n",
    "                \"content\": \"\\n\".join(current_section[\"content\"]).strip(),\n",
    "                \"page\": current_section[\"page\"]\n",
    "            })\n",
    "            # Start new section\n",
    "            current_section = {\"title\": line_stripped, \"content\": [], \"page\": current_page}\n",
    "        else:\n",
    "            current_section[\"content\"].append(line_stripped)\n",
    "            if current_section[\"page\"] == 1 and current_page > 1:\n",
    "                current_section[\"page\"] = current_page\n",
    "    \n",
    "    # Add final section\n",
    "    if current_section[\"content\"]:\n",
    "        sections.append({\n",
    "            \"title\": current_section[\"title\"],\n",
    "            \"content\": \"\\n\".join(current_section[\"content\"]).strip(),\n",
    "            \"page\": current_section[\"page\"]\n",
    "        })\n",
    "    \n",
    "    return sections if sections else [{\"title\": \"Full Document\", \"content\": text, \"page\": 1}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(pdf_path: Path) -> Dict:\n",
    "    \"\"\"Process a single PDF and extract structured content\"\"\"\n",
    "    citation_prefix = get_citation_prefix(pdf_path)\n",
    "    print(f\"Processing: {pdf_path.name} -> {citation_prefix}\")\n",
    "    \n",
    "    full_text = \"\"\n",
    "    pages_content = []\n",
    "    \n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page_num, page in enumerate(pdf.pages, 1):\n",
    "                text = page.extract_text() or \"\"\n",
    "                full_text += f\"\\n\\n--- Page {page_num} ---\\n\\n{text}\"\n",
    "                pages_content.append({\n",
    "                    \"page\": page_num,\n",
    "                    \"text\": text\n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_path.name}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Extract sections\n",
    "    sections = extract_sections(full_text)\n",
    "    \n",
    "    # Create citation mappings\n",
    "    citations = []\n",
    "    section_idx = 1\n",
    "    \n",
    "    for section in sections:\n",
    "        start_page = section.get(\"page\", 1)\n",
    "        citation_id = f\"{citation_prefix}_section{section_idx}_p{start_page}\"\n",
    "        citations.append({\n",
    "            \"citation_id\": citation_id,\n",
    "            \"section_title\": section[\"title\"],\n",
    "            \"section_number\": section_idx,\n",
    "            \"start_page\": start_page,\n",
    "            \"content_preview\": section[\"content\"][:200] + \"...\" if len(section[\"content\"]) > 200 else section[\"content\"]\n",
    "        })\n",
    "        section_idx += 1\n",
    "    \n",
    "    # Clean full_text (remove page markers for cleaner storage)\n",
    "    clean_text = re.sub(r'--- Page \\d+ ---\\n\\n', '\\n', full_text)\n",
    "    \n",
    "    return {\n",
    "        \"source_file\": str(pdf_path.relative_to(BASE_DIR)),\n",
    "        \"citation_prefix\": citation_prefix,\n",
    "        \"total_pages\": len(pages_content),\n",
    "        \"full_text\": clean_text,\n",
    "        \"pages\": pages_content,\n",
    "        \"sections\": sections,\n",
    "        \"citations\": citations,\n",
    "        \"metadata\": {\n",
    "            \"category\": pdf_path.parent.name,\n",
    "            \"filename\": pdf_path.name,\n",
    "            \"word_count\": len(clean_text.split()),\n",
    "            \"char_count\": len(clean_text)\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ics_form_209_incident_status_summary_v3.pdf -> fema_ics209\n",
      "Processing: ics_form_205a_communications_list_v3.pdf -> fema_ics205\n",
      "Processing: ics_form_211_incident_check-in_list_v3.1.pdf -> fema_ics211\n",
      "Processing: ics_form_207_incident_organization_chart_v3.pdf -> fema_ics207\n",
      "Processing: ics_form_213_general_message_v3.pdf -> fema_ics213\n",
      "Processing: ics_form_210_resource_status_change_v3.pdf -> fema_ics210\n",
      "Processing: ics_form_215_operational_planning_worksheet_v3.pdf -> fema_ics215\n",
      "Processing: ics_form_204_assignment_list_v3.1.pdf -> fema_ics204\n",
      "Processing: ics_form_201_incident_briefing_v3.pdf -> fema_ics201\n",
      "Processing: nims-guideline-resource-management-preparedness.pdf -> fema_nims_guideline-resource-management-preparedness\n",
      "Processing: nims_doctrine.pdf -> fema_nims_doctrine\n",
      "Processing: nims-incident-complexity-guide.pdf -> fema_nims_incident-complexity-guide\n",
      "Processing: nrf_finalapproved_2011028.pdf -> fema_nrf\n",
      "Processing: cert_basic_training_manual.pdf -> cert_basic_training_manual\n",
      "Processing: IS0100c_IG.pdf -> 04 IG_is0100c_ig\n",
      "Processing: IS0100c_POI.pdf -> 04 IG_is0100c_poi\n",
      "Processing: ICS_for_Schools.pdf -> 06 Handouts_ics_for_schools\n",
      "Processing: ICS_for_Utilities.pdf -> 06 Handouts_ics_for_utilities\n",
      "Processing: ICS_for_Public_Works.pdf -> 06 Handouts_ics_for_public_works\n",
      "Processing: IS0100c_SM.pdf -> 03 SM_is0100c_sm\n",
      "Processing: National_Incident_Management System_Third Edition_October_2017.pdf -> Handouts_national_incident_management system_third edition_october_2017\n",
      "Processing: POIIS0700b.pdf -> POI_poiis0700b\n",
      "Processing: IGIS0700b.pdf -> Instructor Guide_igis0700b\n",
      "Processing: SMIS0700b.pdf -> Student Manual_smis0700b\n",
      "Processing: IS0200c IG.pdf -> 04 IG_is0200c ig\n",
      "Processing: IS0200c_POI.pdf -> 04 IG_is0200c_poi\n",
      "Processing: Sample Typing (pdf).pdf -> 06 Handouts_sample typing (pdf)\n",
      "Processing: Activity 4.2 - ICS201 Student Version.pdf -> 06 Handouts_activity 4.2 - ics201 student version\n",
      "Processing: IS-200.c Sample Completed ICS Form 201.pdf -> 06 Handouts_is-200.c sample completed ics form 201\n",
      "Processing: IS0200c SM.pdf -> 03 SM_is0200c sm\n",
      "\n",
      "‚úÖ Processed: 49\n",
      "‚ùå Failed: 0\n"
     ]
    }
   ],
   "source": [
    "# Process all PDFs\n",
    "all_pdfs = list(RAW_DIR.rglob(\"*.pdf\"))\n",
    "print(f\"Found {len(all_pdfs)} PDF files to process\\n\")\n",
    "\n",
    "processed_docs = []\n",
    "failed_docs = []\n",
    "\n",
    "for pdf_path in all_pdfs:\n",
    "    result = process_pdf(pdf_path)\n",
    "    if result:\n",
    "        processed_docs.append(result)\n",
    "        \n",
    "        # Save processed version\n",
    "        processed_file = PROCESSED_DIR / f\"{result['citation_prefix']}.json\"\n",
    "        with open(processed_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(result, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata_file = METADATA_DIR / f\"{result['citation_prefix']}_metadata.json\"\n",
    "        metadata = {\n",
    "            \"citation_prefix\": result[\"citation_prefix\"],\n",
    "            \"source_file\": result[\"source_file\"],\n",
    "            \"total_pages\": result[\"total_pages\"],\n",
    "            \"num_sections\": len(result[\"sections\"]),\n",
    "            \"num_citations\": len(result[\"citations\"]),\n",
    "            \"metadata\": result[\"metadata\"]\n",
    "        }\n",
    "        with open(metadata_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "    else:\n",
    "        failed_docs.append(str(pdf_path))\n",
    "\n",
    "print(f\"\\n‚úÖ Processed: {len(processed_docs)}\")\n",
    "print(f\"‚ùå Failed: {len(failed_docs)}\")\n",
    "if failed_docs:\n",
    "    print(\"\\nFailed files:\")\n",
    "    for f in failed_docs:\n",
    "        print(f\"  - {f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Citation index created with 754 entries\n",
      "üìÅ Saved to: /Users/ronny/Desktop/ML projects/DisasterOps/Data/metadata/citation_index.json\n",
      "\n",
      "üìä Summary:\n",
      "  Documents: 49\n",
      "  Pages: 2714\n",
      "  Citations: 754\n",
      "  Words: 541,509\n",
      "  Categories: {'ready_gov': 7, 'ics_forms': 21, 'fema_other': 4, 'cert': 1, '04 IG': 4, '06 Handouts': 6, '03 SM': 2, 'Handouts': 1, 'POI': 1, 'Instructor Guide': 1, 'Student Manual': 1}\n"
     ]
    }
   ],
   "source": [
    "# Create master citation index\n",
    "citation_index = {}\n",
    "for doc in processed_docs:\n",
    "    for citation in doc[\"citations\"]:\n",
    "        citation_index[citation[\"citation_id\"]] = {\n",
    "            \"citation_prefix\": doc[\"citation_prefix\"],\n",
    "            \"source_file\": doc[\"source_file\"],\n",
    "            \"section_title\": citation[\"section_title\"],\n",
    "            \"section_number\": citation[\"section_number\"],\n",
    "            \"start_page\": citation[\"start_page\"],\n",
    "            \"content_preview\": citation[\"content_preview\"]\n",
    "        }\n",
    "\n",
    "# Save citation index\n",
    "index_file = METADATA_DIR / \"citation_index.json\"\n",
    "with open(index_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(citation_index, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úÖ Citation index created with {len(citation_index)} entries\")\n",
    "print(f\"üìÅ Saved to: {index_file}\")\n",
    "\n",
    "# Create summary statistics\n",
    "summary = {\n",
    "    \"total_documents\": len(processed_docs),\n",
    "    \"total_pages\": sum(doc[\"total_pages\"] for doc in processed_docs),\n",
    "    \"total_citations\": len(citation_index),\n",
    "    \"categories\": defaultdict(int),\n",
    "    \"word_count\": sum(doc[\"metadata\"][\"word_count\"] for doc in processed_docs)\n",
    "}\n",
    "\n",
    "for doc in processed_docs:\n",
    "    summary[\"categories\"][doc[\"metadata\"][\"category\"]] += 1\n",
    "\n",
    "summary_file = METADATA_DIR / \"processing_summary.json\"\n",
    "with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"  Documents: {summary['total_documents']}\")\n",
    "print(f\"  Pages: {summary['total_pages']}\")\n",
    "print(f\"  Citations: {summary['total_citations']}\")\n",
    "print(f\"  Words: {summary['word_count']:,}\")\n",
    "print(f\"  Categories: {dict(summary['categories'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG PIPELINE ( Chunking + embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Installing collected packages: rank-bm25, sentence-transformers\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2/2\u001b[0m [sentence-transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed rank-bm25-0.2.2 sentence-transformers-5.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers rank-bm25 torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model...\n",
      "‚úì Model loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "import pickle\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Paths\n",
    "CHUNKS_DIR = BASE_DIR / \"Data\" / \"chunks\"\n",
    "EMBEDDINGS_DIR = BASE_DIR / \"Data\" / \"embeddings\"\n",
    "CHUNKS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "EMBEDDINGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load embedding model (using sentence-transformers as default - no API key needed)\n",
    "print(\"Loading embedding model...\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')  # Fast, good quality, 384 dims\n",
    "print(\"‚úì Model loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, citation_id: str, chunk_size: int = 500, chunk_overlap: int = 50) -> List[Dict]:\n",
    "    \"\"\"Create chunks with overlap and preserve citation mapping\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), chunk_size - chunk_overlap):\n",
    "        chunk_words = words[i:i + chunk_size]\n",
    "        chunk_text = \" \".join(chunk_words)\n",
    "        \n",
    "        if chunk_text.strip():\n",
    "            chunks.append({\n",
    "                \"text\": chunk_text,\n",
    "                \"citation_id\": citation_id,\n",
    "                \"chunk_index\": len(chunks),\n",
    "                \"word_count\": len(chunk_words)\n",
    "            })\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def process_documents_for_rag(processed_docs: List[Dict]) -> Tuple[List[Dict], Dict]:\n",
    "    \"\"\"Create chunks from all processed documents\"\"\"\n",
    "    all_chunks = []\n",
    "    chunk_to_citation = {}\n",
    "    \n",
    "    for doc in processed_docs:\n",
    "        citation_prefix = doc[\"citation_prefix\"]\n",
    "        \n",
    "        # Chunk by sections (hierarchical approach)\n",
    "        for section in doc[\"sections\"]:\n",
    "            section_citation = f\"{citation_prefix}_section{section.get('section_number', 1)}_p{section.get('page', 1)}\"\n",
    "            section_text = section[\"content\"]\n",
    "            \n",
    "            # Further chunk if section is too long\n",
    "            if len(section_text.split()) > 500:\n",
    "                section_chunks = chunk_text(section_text, section_citation)\n",
    "                all_chunks.extend(section_chunks)\n",
    "            else:\n",
    "                all_chunks.append({\n",
    "                    \"text\": section_text,\n",
    "                    \"citation_id\": section_citation,\n",
    "                    \"chunk_index\": 0,\n",
    "                    \"word_count\": len(section_text.split())\n",
    "                })\n",
    "    \n",
    "    # Build citation mapping\n",
    "    for i, chunk in enumerate(all_chunks):\n",
    "        chunk_to_citation[i] = chunk[\"citation_id\"]\n",
    "    \n",
    "    return all_chunks, chunk_to_citation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed documents...\n",
      "Loaded 47 documents\n",
      "\n",
      "Creating chunks...\n",
      "‚úì Created 1694 chunks\n",
      "‚úì Average chunk size: 343 words\n",
      "‚úì Saved chunks to /Users/ronny/Desktop/ML projects/DisasterOps/Data/chunks/all_chunks.json\n"
     ]
    }
   ],
   "source": [
    "# Load processed documents and create chunks\n",
    "print(\"Loading processed documents...\")\n",
    "processed_files = list(PROCESSED_DIR.glob(\"*.json\"))\n",
    "processed_docs = []\n",
    "for f in processed_files:\n",
    "    with open(f, 'r', encoding='utf-8') as file:\n",
    "        processed_docs.append(json.load(file))\n",
    "\n",
    "print(f\"Loaded {len(processed_docs)} documents\")\n",
    "\n",
    "print(\"\\nCreating chunks...\")\n",
    "all_chunks, chunk_to_citation = process_documents_for_rag(processed_docs)\n",
    "\n",
    "print(f\"‚úì Created {len(all_chunks)} chunks\")\n",
    "print(f\"‚úì Average chunk size: {np.mean([c['word_count'] for c in all_chunks]):.0f} words\")\n",
    "\n",
    "# Save chunks\n",
    "chunks_file = CHUNKS_DIR / \"all_chunks.json\"\n",
    "with open(chunks_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_chunks, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "citation_map_file = CHUNKS_DIR / \"chunk_to_citation.json\"\n",
    "with open(citation_map_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(chunk_to_citation, f, indent=2)\n",
    "\n",
    "print(f\"‚úì Saved chunks to {chunks_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9eaffe4279d4ef79158373b6755d142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Generated 1694 embeddings\n",
      "‚úì Embedding dimension: 384\n",
      "‚úì Saved embeddings to /Users/ronny/Desktop/ML projects/DisasterOps/Data/embeddings/embeddings.npy\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings\n",
    "print(\"Generating embeddings...\")\n",
    "chunk_texts = [chunk[\"text\"] for chunk in all_chunks]\n",
    "embeddings = embedding_model.encode(chunk_texts, show_progress_bar=True, batch_size=32)\n",
    "\n",
    "print(f\"‚úì Generated {len(embeddings)} embeddings\")\n",
    "print(f\"‚úì Embedding dimension: {embeddings.shape[1]}\")\n",
    "\n",
    "# Save embeddings\n",
    "embeddings_file = EMBEDDINGS_DIR / \"embeddings.npy\"\n",
    "np.save(embeddings_file, embeddings)\n",
    "print(f\"‚úì Saved embeddings to {embeddings_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build BM25 index (sparse retrieval)\n",
    "print(\"Building BM25 index...\")\n",
    "tokenized_chunks = [chunk[\"text\"].lower().split() for chunk in all_chunks]\n",
    "bm25 = BM25Okapi(tokenized_chunks)\n",
    "\n",
    "print(\"‚úì BM25 index built\")\n",
    "\n",
    "# Save BM25 model\n",
    "bm25_file = EMBEDDINGS_DIR / \"bm25_model.pkl\"\n",
    "with open(bm25_file, 'wb') as f:\n",
    "    pickle.dump(bm25, f)\n",
    "print(f\"‚úì Saved BM25 model to {bm25_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(query: str, embeddings: np.ndarray, bm25: BM25Okapi, chunk_texts: List[str], \n",
    "                  top_k: int = 10, dense_weight: float = 0.5) -> List[Tuple[int, float]]:\n",
    "    \"\"\"Hybrid search combining dense (embeddings) and sparse (BM25) retrieval\"\"\"\n",
    "    # Dense search\n",
    "    query_embedding = embedding_model.encode([query])[0]\n",
    "    dense_scores = np.dot(embeddings, query_embedding)\n",
    "    dense_scores = (dense_scores - dense_scores.min()) / (dense_scores.max() - dense_scores.min() + 1e-8)\n",
    "    \n",
    "    # Sparse search (BM25)\n",
    "    tokenized_query = query.lower().split()\n",
    "    sparse_scores = bm25.get_scores(tokenized_query)\n",
    "    sparse_scores = (sparse_scores - sparse_scores.min()) / (sparse_scores.max() - sparse_scores.min() + 1e-8)\n",
    "    \n",
    "    # Combine scores\n",
    "    hybrid_scores = dense_weight * dense_scores + (1 - dense_weight) * sparse_scores\n",
    "    \n",
    "    # Get top k\n",
    "    top_indices = np.argsort(hybrid_scores)[::-1][:top_k]\n",
    "    results = [(int(idx), float(hybrid_scores[idx])) for idx in top_indices]\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test search\n",
    "test_query = \"How to establish triage area during emergency\"\n",
    "print(f\"Testing search with query: '{test_query}'\")\n",
    "results = hybrid_search(test_query, embeddings, bm25, chunk_texts, top_k=5)\n",
    "\n",
    "print(\"\\nTop 5 results:\")\n",
    "for i, (idx, score) in enumerate(results, 1):\n",
    "    chunk = all_chunks[idx]\n",
    "    print(f\"\\n{i}. Score: {score:.4f}\")\n",
    "    print(f\"   Citation: {chunk['citation_id']}\")\n",
    "    print(f\"   Text: {chunk['text'][:150]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ RAG Pipeline Complete!\n",
      "üìä Summary:\n",
      "   Chunks: 1694\n",
      "   Embeddings: (1694, 384)\n",
      "   BM25 index: Built\n",
      "   Metadata saved to: /Users/ronny/Desktop/ML projects/DisasterOps/Data/embeddings/rag_metadata.json\n"
     ]
    }
   ],
   "source": [
    "# Save RAG pipeline metadata\n",
    "rag_metadata = {\n",
    "    \"total_chunks\": len(all_chunks),\n",
    "    \"embedding_dim\": embeddings.shape[1],\n",
    "    \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
    "    \"chunk_size\": 500,\n",
    "    \"chunk_overlap\": 50,\n",
    "    \"files\": {\n",
    "        \"chunks\": str(chunks_file.relative_to(BASE_DIR)),\n",
    "        \"embeddings\": str(embeddings_file.relative_to(BASE_DIR)),\n",
    "        \"bm25\": str(bm25_file.relative_to(BASE_DIR)),\n",
    "        \"citation_map\": str(citation_map_file.relative_to(BASE_DIR))\n",
    "    }\n",
    "}\n",
    "\n",
    "rag_metadata_file = EMBEDDINGS_DIR / \"rag_metadata.json\"\n",
    "with open(rag_metadata_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(rag_metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ RAG Pipeline Complete!\")\n",
    "print(f\"üìä Summary:\")\n",
    "print(f\"   Chunks: {rag_metadata['total_chunks']}\")\n",
    "print(f\"   Embeddings: {embeddings.shape}\")\n",
    "print(f\"   BM25 index: Built\")\n",
    "print(f\"   Metadata saved to: {rag_metadata_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST 1: Chunk Quality Statistics\n",
      "============================================================\n",
      "\n",
      "Chunk Size Statistics:\n",
      "  Total chunks: 1694\n",
      "  Mean words: 343.0\n",
      "  Median words: 500.0\n",
      "  Min words: 1\n",
      "  Max words: 500\n",
      "  Std dev: 192.1\n",
      "\n",
      "Citation Integrity:\n",
      "  Unique citations: 465\n",
      "  Chunks with citations: 1694/1694\n",
      "\n",
      "Sample Chunks (first 3):\n",
      "\n",
      "1. Citation: fema_ics218_section1_p2\n",
      "   Words: 500\n",
      "   Preview: SUPPORT VEHICLE/EQUIPMENT INVENTORY (ICS 218) 1. Incident Name: 2. Incident Number: 3. Date/Time Pre...\n",
      "\n",
      "2. Citation: fema_ics218_section1_p2\n",
      "   Words: 159\n",
      "   Preview: etc. Agency or Owner Enter the name of the agency or owner of the vehicle or equipment. Operator Nam...\n",
      "\n",
      "3. Citation: 04 IG_is0100c_poi_section1_p1\n",
      "   Words: 142\n",
      "   Preview: March 2025 POI - IS-0100.c: An Introduction to the Incident Command System, ICS 100\n",
      "POI - IS-0100.c:...\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Chunk Quality Statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST 1: Chunk Quality Statistics\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "chunk_sizes = [c['word_count'] for c in all_chunks]\n",
    "print(f\"\\nChunk Size Statistics:\")\n",
    "print(f\"  Total chunks: {len(all_chunks)}\")\n",
    "print(f\"  Mean words: {np.mean(chunk_sizes):.1f}\")\n",
    "print(f\"  Median words: {np.median(chunk_sizes):.1f}\")\n",
    "print(f\"  Min words: {np.min(chunk_sizes)}\")\n",
    "print(f\"  Max words: {np.max(chunk_sizes)}\")\n",
    "print(f\"  Std dev: {np.std(chunk_sizes):.1f}\")\n",
    "\n",
    "# Check citation integrity\n",
    "unique_citations = set(c['citation_id'] for c in all_chunks)\n",
    "print(f\"\\nCitation Integrity:\")\n",
    "print(f\"  Unique citations: {len(unique_citations)}\")\n",
    "print(f\"  Chunks with citations: {sum(1 for c in all_chunks if c.get('citation_id'))}/{len(all_chunks)}\")\n",
    "\n",
    "# Sample chunks\n",
    "print(f\"\\nSample Chunks (first 3):\")\n",
    "for i, chunk in enumerate(all_chunks[:3]):\n",
    "    print(f\"\\n{i+1}. Citation: {chunk['citation_id']}\")\n",
    "    print(f\"   Words: {chunk['word_count']}\")\n",
    "    print(f\"   Preview: {chunk['text'][:100]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TEST 2: Embedding Quality\n",
      "============================================================\n",
      "\n",
      "Embedding Properties:\n",
      "  Shape: (1694, 384)\n",
      "  Mean: -0.0003\n",
      "  Std: 0.0510\n",
      "  Min: -0.2407\n",
      "  Max: 0.2387\n",
      "\n",
      "Semantic Similarity Test:\n",
      "  Chunk 1: SUPPORT VEHICLE/EQUIPMENT INVENTORY (ICS 218) 1. I...\n",
      "  Chunk 2: etc. Agency or Owner Enter the name of the agency ...\n",
      "  Cosine similarity: 0.5661\n",
      "\n",
      "Duplicate Detection:\n",
      "  Unique embedding norms (rounded): 1/1694\n",
      "  Potential duplicates: 1693\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TEST 3: Retrieval Quality\n",
      "============================================================\n",
      "\n",
      "Testing queries with expected content:\n",
      "\n",
      "Query: 'triage area setup'\n",
      "Expected: Should find CERT/medical response content\n",
      "Top 3 results:\n",
      "    1. Score: 0.7367 | cert_basic_training_manual_section1_p26\n",
      "     Text: have received. VPA protects CERT volunteers during a disaster, and volunteers ma...\n",
      "    2. Score: 0.6506 | cert_basic_training_manual_section1_p204\n",
      "     Text: wall/left wall. Keep in mind that every interior space has six sides ‚Äî including...\n",
      "    3. Score: 0.6359 | Instructor Guide_igis0700b_section1_p15\n",
      "     Text: ‚Ä¢ Distances between personnel and resources Lesson 3: NIMS Management Characteri...\n",
      "\n",
      "Query: 'ICS form 201'\n",
      "Expected: Should find ICS-201 form content\n",
      "Top 3 results:\n",
      "    1. Score: 1.0000 | 03 SM_is0200c sm_section1_p132\n",
      "     Text: Response, ICS 200 Visual 38: Activity 4.2: Using ICS Form 201 Activity Purpose: ...\n",
      "    2. Score: 0.9100 | 04 IG_is0200c ig_section1_p165\n",
      "     Text: Incident Command System for Initial Response, ICS 200 Visual 38: Activity 4.2: U...\n",
      "    3. Score: 0.8661 | 03 SM_is0200c sm_section1_p150\n",
      "     Text: work in 30 minutes.\n",
      "Activity 4.2: Using ICS Form 201\n",
      "Activity Purpose: To give s...\n",
      "\n",
      "Query: 'evacuation procedures'\n",
      "Expected: Should find evacuation/disaster response content\n",
      "Top 3 results:\n",
      "    1. Score: 0.9579 | fema_ics_reference_section1_p324\n",
      "     Text: Action Review (AAR) at the end of each assignment. Unresolved lessons learned fr...\n",
      "    2. Score: 0.9231 | ready_gov_caregivers_section1_p15\n",
      "     Text: MAKE A PLAN TO LEAVE\n",
      "As a caregiver, it is essential to be prepared for evacuati...\n",
      "    3. Score: 0.8982 | fema_ics206_section1_p2\n",
      "     Text: travel time by air and ground from the incident to the hospital. ‚Ä¢ Air ‚Ä¢ Ground ...\n",
      "\n",
      "Query: 'medical plan'\n",
      "Expected: Should find ICS-206 or medical planning content\n",
      "Top 3 results:\n",
      "    1. Score: 0.9518 | fema_ics_reference_section1_p202\n",
      "     Text: (Communications Section Chief facilities-related issues. The Logistics Section\n",
      "a...\n",
      "    2. Score: 0.9377 | fema_ics_reference_section1_p43\n",
      "     Text: the Incident Transportation Plan in coordination with the Operations, Planning, ...\n",
      "    3. Score: 0.9250 | fema_ics206_section1_p1\n",
      "     Text: MEDICAL PLAN (ICS 206)\n",
      "1. Incident Name: 2. Operational Period: Date From: Date ...\n",
      "\n",
      "Query: 'wildfire preparation'\n",
      "Expected: Should find wildfire guide content\n",
      "Top 3 results:\n",
      "    1. Score: 1.0000 | ready_gov_wildfire_section1_p1\n",
      "     Text: BE PREPARED FOR A\n",
      "WILDFIRE\n",
      "Wildfires can ruin homes\n",
      "and cause injuries\n",
      "or death ...\n",
      "    2. Score: 0.6923 | ready_gov_wildfire_section1_p2\n",
      "     Text: WHEN A WILDFIRE THREATENS\n",
      "Prepare Survive Be Safe\n",
      "NOW DURING AFTER\n",
      "Sign up for y...\n",
      "    3. Score: 0.6596 | 04 IG_is0200c ig_section1_p9\n",
      "     Text: Fire Academy (NFA), the National Wildfire Coordinating Group (NWCG), the U.S. De...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FIX: Rebuilding Citation Index from Chunks\n",
      "============================================================\n",
      "‚úì Rebuilt citation index: 465 unique citations\n",
      "‚úì Total chunks covered: 1694\n",
      "‚úì Saved to: /Users/ronny/Desktop/ML projects/DisasterOps/Data/chunks/chunk_citation_index.json\n",
      "\n",
      "üìä Content Analysis:\n",
      "  Unique text chunks: 1547/1694\n",
      "  Duplicate texts: 147\n",
      "\n",
      "üìù Note: 'Potential duplicates: 1693' from Test 2 is a FALSE POSITIVE\n",
      "  Sentence-transformers normalizes embeddings to unit length (norm ‚âà 1.0)\n",
      "  This is expected behavior, not actual duplicates.\n"
     ]
    }
   ],
   "source": [
    "# FIX: Rebuild citation index from chunks\n",
    "print(\"=\" * 60)\n",
    "print(\"FIX: Rebuilding Citation Index from Chunks\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Build citation index from CHUNKS (not original documents)\n",
    "chunk_citation_index = {}\n",
    "for chunk in all_chunks:\n",
    "    cit_id = chunk['citation_id']\n",
    "    if cit_id not in chunk_citation_index:\n",
    "        chunk_citation_index[cit_id] = {\n",
    "            \"citation_id\": cit_id,\n",
    "            \"chunk_count\": 0,\n",
    "            \"sample_text\": chunk['text'][:150] + \"...\"\n",
    "        }\n",
    "    chunk_citation_index[cit_id][\"chunk_count\"] += 1\n",
    "\n",
    "# Save updated citation index\n",
    "updated_index_file = CHUNKS_DIR / \"chunk_citation_index.json\"\n",
    "with open(updated_index_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(chunk_citation_index, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úì Rebuilt citation index: {len(chunk_citation_index)} unique citations\")\n",
    "print(f\"‚úì Total chunks covered: {len(all_chunks)}\")\n",
    "print(f\"‚úì Saved to: {updated_index_file}\")\n",
    "\n",
    "# Check for content duplicates (actual issue)\n",
    "unique_texts = len(set(c['text'] for c in all_chunks))\n",
    "print(f\"\\nüìä Content Analysis:\")\n",
    "print(f\"  Unique text chunks: {unique_texts}/{len(all_chunks)}\")\n",
    "print(f\"  Duplicate texts: {len(all_chunks) - unique_texts}\")\n",
    "\n",
    "# Note about embedding norms\n",
    "print(f\"\\nüìù Note: 'Potential duplicates: 1693' from Test 2 is a FALSE POSITIVE\")\n",
    "print(f\"  Sentence-transformers normalizes embeddings to unit length (norm ‚âà 1.0)\")\n",
    "print(f\"  This is expected behavior, not actual duplicates.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PIPELINE RESULT SUMMARY\n",
      "================================================================================\n",
      "\n",
      "üìã Structured Incident:\n",
      "{\n",
      "  \"hazards\": [\n",
      "    \"flood\"\n",
      "  ],\n",
      "  \"injuries\": [],\n",
      "  \"infrastructure_status\": [\n",
      "    \"buildings affected\"\n",
      "  ],\n",
      "  \"weather\": \"water rising\",\n",
      "  \"available_responders\": [],\n",
      "  \"constraints\": []\n",
      "}\n",
      "\n",
      "üìö Evidence: 15 chunks retrieved\n",
      "   Unique citations: 15\n",
      "\n",
      "üìù Operational Plan:\n",
      "   Objectives: 4\n",
      "   Tasks: 5\n",
      "   Time Horizon: 0-2 hours\n",
      "\n",
      "üì£ Communications:\n",
      "   Public Advisory: Attention residents: Due to rising water levels, flooding is expected in our area. Please stay indoors and avoid flooded roads. If you live in a low-l...\n",
      "\n",
      "‚úÖ Verification:\n",
      "   Citation Coverage: all_claims_cited\n",
      "   Confidence Score: 0.90\n",
      "   Multi-Source Status: validated\n",
      "   Flagged Issues: 1\n",
      "   Known Claims: 4\n",
      "   Unknown Claims: 1\n"
     ]
    }
   ],
   "source": [
    "from agents import run_pipeline, print_result_summary\n",
    "\n",
    "result = run_pipeline(\"Flood in downtown. Water rising. Buildings affected.\")\n",
    "print_result_summary(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPREHENSIVE PIPELINE TEST: 5-AGENT SYSTEM\n",
      "================================================================================\n",
      "Testing 8 scenarios...\n",
      "\n",
      "[1/8] Processing: Urban Flooding\n",
      "--------------------------------------------------------------------------------\n",
      "  ‚úì Processed in 34.15s\n",
      "  Hazards: flood\n",
      "  Evidence: 15 chunks, 15 citations\n",
      "  Confidence: 0.90\n",
      "  Citation Coverage: all_claims_cited\n",
      "\n",
      "[2/8] Processing: Wildfire Smoke\n",
      "--------------------------------------------------------------------------------\n",
      "  ‚úì Processed in 42.47s\n",
      "  Hazards: wildfire, heavy smoke, poor air quality\n",
      "  Evidence: 15 chunks, 15 citations\n",
      "  Confidence: 0.90\n",
      "  Citation Coverage: all_claims_cited\n",
      "\n",
      "[3/8] Processing: Earthquake with Aftershocks\n",
      "--------------------------------------------------------------------------------\n",
      "  ‚úì Processed in 34.52s\n",
      "  Hazards: earthquake, aftershocks\n",
      "  Evidence: 15 chunks, 15 citations\n",
      "  Confidence: 0.90\n",
      "  Citation Coverage: all_claims_cited\n",
      "\n",
      "[4/8] Processing: Hurricane Landfall\n",
      "--------------------------------------------------------------------------------\n",
      "  ‚úì Processed in 39.91s\n",
      "  Hazards: hurricane, high winds, storm surge\n",
      "  Evidence: 15 chunks, 15 citations\n",
      "  Confidence: 0.95\n",
      "  Citation Coverage: all_claims_cited\n",
      "\n",
      "[5/8] Processing: Chemical Spill\n",
      "--------------------------------------------------------------------------------\n",
      "  ‚úì Processed in 34.09s\n",
      "  Hazards: chemical spill\n",
      "  Evidence: 15 chunks, 15 citations\n",
      "  Confidence: 0.90\n",
      "  Citation Coverage: all_claims_cited\n",
      "\n",
      "[6/8] Processing: Tornado Touchdown\n",
      "--------------------------------------------------------------------------------\n",
      "  ‚úì Processed in 37.57s\n",
      "  Hazards: tornado, downed power lines\n",
      "  Evidence: 15 chunks, 15 citations\n",
      "  Confidence: 0.90\n",
      "  Citation Coverage: all_claims_cited\n",
      "\n",
      "[7/8] Processing: Winter Storm\n",
      "--------------------------------------------------------------------------------\n",
      "  ‚úì Processed in 35.16s\n",
      "  Hazards: heavy snow, ice, downed power lines\n",
      "  Evidence: 15 chunks, 15 citations\n",
      "  Confidence: 0.90\n",
      "  Citation Coverage: all_claims_cited\n",
      "\n",
      "[8/8] Processing: Multi-Hazard: Flood + Landslide\n",
      "--------------------------------------------------------------------------------\n",
      "  ‚úì Processed in 42.30s\n",
      "  Hazards: flood, landslide\n",
      "  Evidence: 15 chunks, 15 citations\n",
      "  Confidence: 0.95\n",
      "  Citation Coverage: all_claims_cited\n",
      "\n",
      "================================================================================\n",
      "TEST SUMMARY\n",
      "================================================================================\n",
      "Total: 8 | Successful: 8 | Failed: 0\n",
      "Total time: 300.17s | Avg per scenario: 37.52s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'Urban Flooding',\n",
       "  'success': True,\n",
       "  'time': 34.153101682662964,\n",
       "  'hazards': ['flood'],\n",
       "  'evidence': 15,\n",
       "  'citations': 15,\n",
       "  'confidence': 0.9,\n",
       "  'coverage': 'all_claims_cited'},\n",
       " {'name': 'Wildfire Smoke',\n",
       "  'success': True,\n",
       "  'time': 42.4677939414978,\n",
       "  'hazards': ['wildfire', 'heavy smoke', 'poor air quality'],\n",
       "  'evidence': 15,\n",
       "  'citations': 15,\n",
       "  'confidence': 0.9,\n",
       "  'coverage': 'all_claims_cited'},\n",
       " {'name': 'Earthquake with Aftershocks',\n",
       "  'success': True,\n",
       "  'time': 34.52307915687561,\n",
       "  'hazards': ['earthquake', 'aftershocks'],\n",
       "  'evidence': 15,\n",
       "  'citations': 15,\n",
       "  'confidence': 0.9,\n",
       "  'coverage': 'all_claims_cited'},\n",
       " {'name': 'Hurricane Landfall',\n",
       "  'success': True,\n",
       "  'time': 39.907214879989624,\n",
       "  'hazards': ['hurricane', 'high winds', 'storm surge'],\n",
       "  'evidence': 15,\n",
       "  'citations': 15,\n",
       "  'confidence': 0.95,\n",
       "  'coverage': 'all_claims_cited'},\n",
       " {'name': 'Chemical Spill',\n",
       "  'success': True,\n",
       "  'time': 34.09149718284607,\n",
       "  'hazards': ['chemical spill'],\n",
       "  'evidence': 15,\n",
       "  'citations': 15,\n",
       "  'confidence': 0.9,\n",
       "  'coverage': 'all_claims_cited'},\n",
       " {'name': 'Tornado Touchdown',\n",
       "  'success': True,\n",
       "  'time': 37.57206606864929,\n",
       "  'hazards': ['tornado', 'downed power lines'],\n",
       "  'evidence': 15,\n",
       "  'citations': 15,\n",
       "  'confidence': 0.9,\n",
       "  'coverage': 'all_claims_cited'},\n",
       " {'name': 'Winter Storm',\n",
       "  'success': True,\n",
       "  'time': 35.15569591522217,\n",
       "  'hazards': ['heavy snow', 'ice', 'downed power lines'],\n",
       "  'evidence': 15,\n",
       "  'citations': 15,\n",
       "  'confidence': 0.9,\n",
       "  'coverage': 'all_claims_cited'},\n",
       " {'name': 'Multi-Hazard: Flood + Landslide',\n",
       "  'success': True,\n",
       "  'time': 42.29970932006836,\n",
       "  'hazards': ['flood', 'landslide'],\n",
       "  'evidence': 15,\n",
       "  'citations': 15,\n",
       "  'confidence': 0.95,\n",
       "  'coverage': 'all_claims_cited'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or run tests\n",
    "from agents import run_test_suite\n",
    "run_test_suite()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Exported JSON to outputs/my_incident_fixed.json\n"
     ]
    }
   ],
   "source": [
    "from agents import run_pipeline\n",
    "from output_generation import generate_from_agent_output, export_to_json\n",
    "from pathlib import Path\n",
    "\n",
    "# Run pipeline\n",
    "result = run_pipeline(\"Flood in downtown area. Multiple buildings affected.\")\n",
    "\n",
    "# Generate outputs (this will use the fixed code)\n",
    "all_outputs = generate_from_agent_output(\n",
    "    result,\n",
    "    incident_name=\"Urban Flooding\",\n",
    "    incident_number=\"2024-001\"\n",
    ")\n",
    "\n",
    "# Export\n",
    "export_to_json(all_outputs, Path(\"outputs/my_incident_fixed.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-openai\n",
      "  Using cached langchain_openai-1.1.6-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting langchain-core\n",
      "  Using cached langchain_core-1.2.5-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting langgraph\n",
      "  Using cached langgraph-1.0.5-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting openai\n",
      "  Using cached openai-2.14.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting tiktoken<1.0.0,>=0.7.0 (from langchain-openai)\n",
      "  Using cached tiktoken-0.12.0-cp314-cp314-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting jsonpatch<2.0.0,>=1.33.0 (from langchain-core)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langsmith<1.0.0,>=0.3.45 (from langchain-core)\n",
      "  Using cached langsmith-0.5.2-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in ./.venv/lib/python3.14/site-packages (from langchain-core) (25.0)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain-core)\n",
      "  Using cached pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in ./.venv/lib/python3.14/site-packages (from langchain-core) (6.0.3)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core)\n",
      "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in ./.venv/lib/python3.14/site-packages (from langchain-core) (4.15.0)\n",
      "Collecting uuid-utils<1.0,>=0.12.0 (from langchain-core)\n",
      "  Using cached uuid_utils-0.12.0-cp39-abi3-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.14/site-packages (from openai) (4.12.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.14/site-packages (from openai) (0.28.1)\n",
      "Collecting jiter<1,>=0.10.0 (from openai)\n",
      "  Using cached jiter-0.12.0-cp314-cp314-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting sniffio (from openai)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>4 in ./.venv/lib/python3.14/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.14/site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.14/site-packages (from httpx<1,>=0.23.0->openai) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.14/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.14/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.14/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
      "Collecting orjson>=3.9.14 (from langsmith<1.0.0,>=0.3.45->langchain-core)\n",
      "  Using cached orjson-3.11.5-cp314-cp314-macosx_15_0_arm64.whl.metadata (41 kB)\n",
      "Collecting requests-toolbelt>=1.0.0 (from langsmith<1.0.0,>=0.3.45->langchain-core)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: requests>=2.0.0 in ./.venv/lib/python3.14/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (2.32.5)\n",
      "Collecting zstandard>=0.23.0 (from langsmith<1.0.0,>=0.3.45->langchain-core)\n",
      "  Using cached zstandard-0.25.0-cp314-cp314-macosx_11_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain-core)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic<3.0.0,>=2.7.4->langchain-core)\n",
      "  Using cached pydantic_core-2.41.5-cp314-cp314-macosx_11_0_arm64.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic<3.0.0,>=2.7.4->langchain-core)\n",
      "  Using cached typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.venv/lib/python3.14/site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.11.3)\n",
      "Collecting langgraph-checkpoint<4.0.0,>=2.1.0 (from langgraph)\n",
      "  Using cached langgraph_checkpoint-3.0.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting langgraph-prebuilt<1.1.0,>=1.0.2 (from langgraph)\n",
      "  Using cached langgraph_prebuilt-1.0.5-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting langgraph-sdk<0.4.0,>=0.3.0 (from langgraph)\n",
      "  Using cached langgraph_sdk-0.3.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting xxhash>=3.5.0 (from langgraph)\n",
      "  Using cached xxhash-3.6.0-cp314-cp314-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting ormsgpack>=1.12.0 (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph)\n",
      "  Using cached ormsgpack-1.12.1-cp314-cp314-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.14/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.14/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core) (2.6.2)\n",
      "Using cached langchain_openai-1.1.6-py3-none-any.whl (84 kB)\n",
      "Using cached langchain_core-1.2.5-py3-none-any.whl (484 kB)\n",
      "Using cached openai-2.14.0-py3-none-any.whl (1.1 MB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached jiter-0.12.0-cp314-cp314-macosx_11_0_arm64.whl (318 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached langsmith-0.5.2-py3-none-any.whl (283 kB)\n",
      "Using cached pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Using cached pydantic_core-2.41.5-cp314-cp314-macosx_11_0_arm64.whl (1.9 MB)\n",
      "Using cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Using cached tiktoken-0.12.0-cp314-cp314-macosx_11_0_arm64.whl (993 kB)\n",
      "Using cached uuid_utils-0.12.0-cp39-abi3-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl (603 kB)\n",
      "Using cached langgraph-1.0.5-py3-none-any.whl (157 kB)\n",
      "Using cached langgraph_checkpoint-3.0.1-py3-none-any.whl (46 kB)\n",
      "Using cached langgraph_prebuilt-1.0.5-py3-none-any.whl (35 kB)\n",
      "Using cached langgraph_sdk-0.3.1-py3-none-any.whl (66 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached orjson-3.11.5-cp314-cp314-macosx_15_0_arm64.whl (129 kB)\n",
      "Using cached ormsgpack-1.12.1-cp314-cp314-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl (376 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Using cached typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Using cached xxhash-3.6.0-cp314-cp314-macosx_11_0_arm64.whl (30 kB)\n",
      "Using cached zstandard-0.25.0-cp314-cp314-macosx_11_0_arm64.whl (640 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: zstandard, xxhash, uuid-utils, typing-inspection, tenacity, sniffio, pydantic-core, ormsgpack, orjson, jsonpatch, jiter, distro, annotated-types, tiktoken, requests-toolbelt, pydantic, openai, langsmith, langgraph-sdk, langchain-core, langgraph-checkpoint, langchain-openai, langgraph-prebuilt, langgraph\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24/24\u001b[0m [langgraph]24\u001b[0m [langgraph]core]\n",
      "\u001b[1A\u001b[2KSuccessfully installed annotated-types-0.7.0 distro-1.9.0 jiter-0.12.0 jsonpatch-1.33 langchain-core-1.2.5 langchain-openai-1.1.6 langgraph-1.0.5 langgraph-checkpoint-3.0.1 langgraph-prebuilt-1.0.5 langgraph-sdk-0.3.1 langsmith-0.5.2 openai-2.14.0 orjson-3.11.5 ormsgpack-1.12.1 pydantic-2.12.5 pydantic-core-2.41.5 requests-toolbelt-1.0.0 sniffio-1.3.1 tenacity-9.1.2 tiktoken-0.12.0 typing-inspection-0.4.2 uuid-utils-0.12.0 xxhash-3.6.0 zstandard-0.25.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-openai langchain-core langgraph openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ronny/Desktop/ML projects/DisasterOps/.venv/lib/python3.14/site-packages/langchain_core/_api/deprecation.py:26: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n",
      "\n",
      "================================================================================\n",
      "OUTPUT GENERATION MODULE - COMPREHENSIVE TEST SUITE\n",
      "================================================================================\n",
      "================================================================================\n",
      "TEST: Single Scenario Output Generation\n",
      "================================================================================\n",
      "\n",
      "Scenario: Urban Flooding\n",
      "Description: Flood in downtown area. Water level rising. Multiple buildings affected. No injuries reported yet. Rain expected to continue.\n",
      "\n",
      "Step 1: Running agent pipeline...\n",
      "\n",
      "================================================================================\n",
      "‚ùå TEST FAILED\n",
      "================================================================================\n",
      "\n",
      "Error: OPENAI_API_KEY environment variable not set\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ronny/Desktop/ML projects/DisasterOps/test_output_generation.py\", line 250, in main\n",
      "    all_outputs, agent_result = test_single_scenario()\n",
      "                                ~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/Users/ronny/Desktop/ML projects/DisasterOps/test_output_generation.py\", line 41, in test_single_scenario\n",
      "    agent_result = run_pipeline(scenario[\"description\"])\n",
      "  File \"/Users/ronny/Desktop/ML projects/DisasterOps/agents.py\", line 598, in run_pipeline\n",
      "    result = app.invoke(initial_state)\n",
      "  File \"/Users/ronny/Desktop/ML projects/DisasterOps/.venv/lib/python3.14/site-packages/langgraph/pregel/main.py\", line 3068, in invoke\n",
      "    for chunk in self.stream(\n",
      "                 ~~~~~~~~~~~^\n",
      "        input,\n",
      "        ^^^^^^\n",
      "    ...<10 lines>...\n",
      "        **kwargs,\n",
      "        ^^^^^^^^^\n",
      "    ):\n",
      "    ^\n",
      "  File \"/Users/ronny/Desktop/ML projects/DisasterOps/.venv/lib/python3.14/site-packages/langgraph/pregel/main.py\", line 2643, in stream\n",
      "    for _ in runner.tick(\n",
      "             ~~~~~~~~~~~^\n",
      "        [t for t in loop.tasks.values() if not t.writes],\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<2 lines>...\n",
      "        schedule_task=loop.accept_push,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ):\n",
      "    ^\n",
      "  File \"/Users/ronny/Desktop/ML projects/DisasterOps/.venv/lib/python3.14/site-packages/langgraph/pregel/_runner.py\", line 167, in tick\n",
      "    run_with_retry(\n",
      "    ~~~~~~~~~~~~~~^\n",
      "        t,\n",
      "        ^^\n",
      "    ...<10 lines>...\n",
      "        },\n",
      "        ^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/ronny/Desktop/ML projects/DisasterOps/.venv/lib/python3.14/site-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n",
      "    return task.proc.invoke(task.input, config)\n",
      "           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ronny/Desktop/ML projects/DisasterOps/.venv/lib/python3.14/site-packages/langgraph/_internal/_runnable.py\", line 656, in invoke\n",
      "    input = context.run(step.invoke, input, config, **kwargs)\n",
      "  File \"/Users/ronny/Desktop/ML projects/DisasterOps/.venv/lib/python3.14/site-packages/langgraph/_internal/_runnable.py\", line 400, in invoke\n",
      "    ret = self.func(*args, **kwargs)\n",
      "  File \"/Users/ronny/Desktop/ML projects/DisasterOps/agents.py\", line 197, in intake_agent\n",
      "    llm = get_llm()\n",
      "  File \"/Users/ronny/Desktop/ML projects/DisasterOps/agents.py\", line 70, in get_llm\n",
      "    raise ValueError(\"OPENAI_API_KEY environment variable not set\")\n",
      "ValueError: OPENAI_API_KEY environment variable not set\n",
      "During task with name 'intake' and id '2be85b63-0330-09db-add3-266b2a6835d7'\n"
     ]
    }
   ],
   "source": [
    "!python test_output_generation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
